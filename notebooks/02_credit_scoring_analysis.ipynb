{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# An√°lisis de Scoring Crediticio - Proyecto de Grado\n",
        "## Framework de RL para Optimizaci√≥n de Datos Sint√©ticos y Modelos ML\n",
        "\n",
        "**T√≠tulo:** Sintetizando datos tabulares: Un Framework de Aprendizaje por Refuerzo para el Benchmark de Datos Sint√©ticos y su Impacto en problemas de Clasificaci√≥n\n",
        "\n",
        "**Autor:** Carlos Andres Cortez Ballen\n",
        "\n",
        "### Objetivos del Proyecto:\n",
        "- Implementar y comparar generadores de datos sint√©ticos (GANs vs m√©todos tradicionales)\n",
        "- Construir framework de RL para selecci√≥n √≥ptima de generadores y modelos ML\n",
        "- Evaluar rendimiento en scoring crediticio para segmento D (baja transaccionalidad)\n",
        "- Desarrollar m√©tricas de calidad para datos sint√©ticos\n",
        "- Realizar benchmarking cruzado entre estrategias\n",
        "\n",
        "### Objetivos de este Notebook:\n",
        "- Explorar datos del segmento D (5K muestras vs 200K del segmento A)\n",
        "- Analizar caracter√≠sticas del scoring crediticio de corto plazo\n",
        "- Identificar patrones y distribuciones en datos financieros\n",
        "- Preparar datos para generaci√≥n sint√©tica\n",
        "- Establecer baseline de performance\n",
        "\n",
        "### Contenido:\n",
        "1. Configuraci√≥n del entorno y librer√≠as\n",
        "2. Carga de datos (UCI + datos internos)\n",
        "3. An√°lisis exploratorio del segmento D\n",
        "4. Comparaci√≥n con segmento A (referencia)\n",
        "5. An√°lisis de variables de scoring crediticio\n",
        "6. Preparaci√≥n para generaci√≥n de datos sint√©ticos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importaci√≥n de librer√≠as y configuraci√≥n\n",
        "import sys\n",
        "import os\n",
        "sys.path.append('..')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "import logging\n",
        "\n",
        "# Configuraci√≥n de visualizaciones\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuraci√≥n de pandas\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Configuraci√≥n de logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Cargar configuraci√≥n\n",
        "with open('../configs/config.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "print(\"‚úÖ Librer√≠as importadas correctamente\")\n",
        "print(f\"üìä Pandas version: {pd.__version__}\")\n",
        "print(f\"üî¢ NumPy version: {np.__version__}\")\n",
        "print(f\"üìà Matplotlib version: {plt.matplotlib.__version__}\")\n",
        "print(f\"üé® Seaborn version: {sns.__version__}\")\n",
        "print(f\"‚öôÔ∏è Configuraci√≥n cargada: {len(config)} secciones\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importar m√≥dulos del proyecto\n",
        "from src.data.data_loader import DataLoader\n",
        "from src.data.data_preprocessor import DataPreprocessor\n",
        "from src.data.data_splitter import DataSplitter\n",
        "from src.data.data_validator import DataValidator\n",
        "\n",
        "# Inicializar componentes\n",
        "data_loader = DataLoader(config)\n",
        "data_preprocessor = DataPreprocessor(config)\n",
        "data_splitter = DataSplitter(config)\n",
        "data_validator = DataValidator(config)\n",
        "\n",
        "print(\"‚úÖ M√≥dulos del proyecto importados correctamente\")\n",
        "print(\"üìÅ DataLoader inicializado\")\n",
        "print(\"üîß DataPreprocessor inicializado\")\n",
        "print(\"‚úÇÔ∏è DataSplitter inicializado\")\n",
        "print(\"‚úÖ DataValidator inicializado\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Carga de Datos de Cr√©dito\n",
        "\n",
        "Vamos a cargar datasets de cr√©dito desde UCI Repository para simular el an√°lisis del segmento D. Estos datasets nos permitir√°n:\n",
        "\n",
        "1. **German Credit Data**: Dataset cl√°sico de scoring crediticio\n",
        "2. **Australian Credit Approval**: Datos de aprobaci√≥n de cr√©dito\n",
        "3. **Credit Card Default**: Datos de default de tarjetas de cr√©dito\n",
        "\n",
        "Estos datasets nos servir√°n como proxy para entender las caracter√≠sticas del segmento D.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar datasets de cr√©dito desde UCI\n",
        "print(\"üîÑ Cargando datasets de cr√©dito desde UCI Repository...\")\n",
        "\n",
        "# Cargar German Credit Data (ID: 144)\n",
        "try:\n",
        "    german_features, german_targets = data_loader.load_uci_dataset(144, \"german_credit\")\n",
        "    german_data = pd.concat([german_features, german_targets], axis=1)\n",
        "    print(f\"‚úÖ German Credit Data cargado: {german_data.shape}\")\n",
        "    print(f\"   Features: {german_features.shape[1]}\")\n",
        "    print(f\"   Target distribution: {german_targets.iloc[:, 0].value_counts().to_dict()}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error cargando German Credit: {e}\")\n",
        "    german_data = None\n",
        "\n",
        "# Cargar Australian Credit Approval (ID: 45)\n",
        "try:\n",
        "    australian_features, australian_targets = data_loader.load_uci_dataset(45, \"australian_credit\")\n",
        "    australian_data = pd.concat([australian_features, australian_targets], axis=1)\n",
        "    print(f\"‚úÖ Australian Credit Data cargado: {australian_data.shape}\")\n",
        "    print(f\"   Features: {australian_features.shape[1]}\")\n",
        "    print(f\"   Target distribution: {australian_targets.iloc[:, 0].value_counts().to_dict()}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error cargando Australian Credit: {e}\")\n",
        "    australian_data = None\n",
        "\n",
        "# Cargar Credit Card Default (ID: 300)\n",
        "try:\n",
        "    default_features, default_targets = data_loader.load_uci_dataset(300, \"credit_card_default\")\n",
        "    default_data = pd.concat([default_features, default_targets], axis=1)\n",
        "    print(f\"‚úÖ Credit Card Default Data cargado: {default_data.shape}\")\n",
        "    print(f\"   Features: {default_features.shape[1]}\")\n",
        "    print(f\"   Target distribution: {default_targets.iloc[:, 0].value_counts().to_dict()}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error cargando Credit Card Default: {e}\")\n",
        "    default_data = None\n",
        "\n",
        "print(\"\\nüìä Resumen de datasets cargados:\")\n",
        "datasets_info = {\n",
        "    'German Credit': german_data.shape if german_data is not None else \"No disponible\",\n",
        "    'Australian Credit': australian_data.shape if australian_data is not None else \"No disponible\", \n",
        "    'Credit Card Default': default_data.shape if default_data is not None else \"No disponible\"\n",
        "}\n",
        "\n",
        "for name, shape in datasets_info.items():\n",
        "    print(f\"   {name}: {shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Seleccionar dataset principal para an√°lisis (German Credit como proxy del segmento D)\n",
        "if german_data is not None:\n",
        "    # Usar German Credit como dataset principal\n",
        "    segment_d_data = german_data.copy()\n",
        "    segment_d_features = german_features.copy()\n",
        "    segment_d_targets = german_targets.copy()\n",
        "    \n",
        "    print(\"üéØ Usando German Credit Data como proxy del Segmento D\")\n",
        "    print(f\"   Tama√±o: {segment_d_data.shape[0]} muestras\")\n",
        "    print(f\"   Features: {segment_d_data.shape[1] - 1}\")\n",
        "    print(f\"   Target: {segment_d_targets.columns[0]}\")\n",
        "    \n",
        "    # Mostrar primeras filas\n",
        "    print(\"\\nüìñ Primeras 5 filas del dataset:\")\n",
        "    display(segment_d_data.head())\n",
        "    \n",
        "    # Informaci√≥n b√°sica del dataset\n",
        "    print(\"\\nüìä Informaci√≥n del dataset:\")\n",
        "    print(segment_d_data.info())\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No se pudo cargar el dataset principal\")\n",
        "    segment_d_data = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. An√°lisis Exploratorio del Segmento D\n",
        "\n",
        "Ahora vamos a realizar un an√°lisis exploratorio completo del dataset que simula el segmento D. Este an√°lisis incluir√°:\n",
        "\n",
        "1. **An√°lisis de calidad de datos**\n",
        "2. **Estad√≠sticas descriptivas**\n",
        "3. **An√°lisis de distribuciones**\n",
        "4. **An√°lisis de correlaciones**\n",
        "5. **Identificaci√≥n de patrones**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# An√°lisis de calidad de datos\n",
        "if segment_d_data is not None:\n",
        "    print(\"üîç AN√ÅLISIS DE CALIDAD DE DATOS - SEGMENTO D\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Validar calidad de datos\n",
        "    quality_results = data_validator.validate_data_quality(segment_d_data, target_col='class')\n",
        "    \n",
        "    # Mostrar resumen de calidad\n",
        "    print(\"\\nüìä RESUMEN DE CALIDAD:\")\n",
        "    print(f\"   Shape: {quality_results['basic_info']['shape']}\")\n",
        "    print(f\"   Memory usage: {quality_results['basic_info']['memory_usage'] / 1024**2:.2f} MB\")\n",
        "    \n",
        "    # Valores faltantes\n",
        "    missing_summary = quality_results['missing_values']['summary']\n",
        "    print(f\"   Missing values: {missing_summary['total_missing']} ({missing_summary['total_percentage']:.2f}%)\")\n",
        "    \n",
        "    # Duplicados\n",
        "    duplicates = quality_results['duplicates']\n",
        "    print(f\"   Duplicates: {duplicates['count']} ({duplicates['percentage']:.2f}%)\")\n",
        "    \n",
        "    # Tipos de datos\n",
        "    data_types = quality_results['data_types']\n",
        "    numeric_cols = [col for col, info in data_types.items() if 'int' in info['dtype'] or 'float' in info['dtype']]\n",
        "    categorical_cols = [col for col, info in data_types.items() if 'object' in info['dtype']]\n",
        "    \n",
        "    print(f\"   Numeric columns: {len(numeric_cols)}\")\n",
        "    print(f\"   Categorical columns: {len(categorical_cols)}\")\n",
        "    \n",
        "    print(\"\\n‚úÖ An√°lisis de calidad completado\")\n",
        "else:\n",
        "    print(\"‚ùå No hay datos disponibles para an√°lisis de calidad\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
