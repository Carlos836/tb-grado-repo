{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experimentaci√≥n con Modelos ML - Proyecto de Grado\n",
        "## Framework de RL para Optimizaci√≥n de Datos Sint√©ticos y Modelos ML\n",
        "\n",
        "**T√≠tulo:** Sintetizando datos tabulares: Un Framework de Aprendizaje por Refuerzo para el Benchmark de Datos Sint√©ticos y su Impacto en problemas de Clasificaci√≥n\n",
        "\n",
        "**Autor:** Carlos Andres Cortez Ballen\n",
        "\n",
        "### Objetivos de este Notebook:\n",
        "- Implementar y entrenar modelos de scoring crediticio (XGBoost, CatBoost, LightGBM, etc.)\n",
        "- Evaluar modelos con m√©tricas espec√≠ficas del dominio (AUC, PSI, Traffic Light)\n",
        "- Comparar performance entre modelos entrenados con datos reales vs sint√©ticos\n",
        "- Establecer baseline de performance para el segmento D\n",
        "- Preparar datos para el framework de RL\n",
        "\n",
        "### Metodolog√≠a:\n",
        "1. **Modelos ML**: XGBoost, CatBoost, LightGBM, HistGradientBoosting, RandomForest, LogisticRegression\n",
        "2. **M√©tricas de Evaluaci√≥n**: AUC-ROC, PSI, Traffic Light, Gini Coefficient, Population Stability\n",
        "3. **Validaci√≥n**: Cross-validation y evaluaci√≥n en datos de prueba\n",
        "4. **Comparaci√≥n**: Datos reales vs datos sint√©ticos\n",
        "\n",
        "### Contenido:\n",
        "1. Configuraci√≥n y carga de datos\n",
        "2. Divisi√≥n de datos (train/validation/test)\n",
        "3. Entrenamiento de modelos con datos reales\n",
        "4. Evaluaci√≥n y comparaci√≥n de modelos\n",
        "5. An√°lisis de importancia de features\n",
        "6. Preparaci√≥n para experimentos con datos sint√©ticos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importaci√≥n de librer√≠as y configuraci√≥n\n",
        "import sys\n",
        "import os\n",
        "sys.path.append('..')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "import yaml\n",
        "import logging\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "# Configuraci√≥n de visualizaciones\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configuraci√≥n de pandas\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "# Configuraci√≥n de logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Cargar configuraci√≥n\n",
        "with open('../configs/config.yaml', 'r') as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "print(\"‚úÖ Librer√≠as importadas correctamente\")\n",
        "print(f\"üìä Pandas version: {pd.__version__}\")\n",
        "print(f\"üî¢ NumPy version: {np.__version__}\")\n",
        "print(f\"üìà Matplotlib version: {plt.matplotlib.__version__}\")\n",
        "print(f\"üé® Seaborn version: {sns.__version__}\")\n",
        "print(f\"‚öôÔ∏è Configuraci√≥n cargada: {len(config)} secciones\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Importar m√≥dulos del proyecto\n",
        "from src.data.data_loader import DataLoader\n",
        "from src.data.data_preprocessor import DataPreprocessor\n",
        "from src.data.data_splitter import DataSplitter\n",
        "from src.models.model_factory import ModelFactory\n",
        "from src.models.model_evaluator import CreditModelEvaluator\n",
        "\n",
        "# Inicializar componentes\n",
        "data_loader = DataLoader(config)\n",
        "data_preprocessor = DataPreprocessor(config)\n",
        "data_splitter = DataSplitter(config)\n",
        "model_factory = ModelFactory(config)\n",
        "credit_evaluator = CreditModelEvaluator(config)\n",
        "\n",
        "print(\"‚úÖ M√≥dulos del proyecto importados correctamente\")\n",
        "print(\"üìÅ DataLoader inicializado\")\n",
        "print(\"üîß DataPreprocessor inicializado\")\n",
        "print(\"‚úÇÔ∏è DataSplitter inicializado\")\n",
        "print(\"ü§ñ ModelFactory inicializado\")\n",
        "print(\"üìä CreditModelEvaluator inicializado\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar y preparar datos del segmento D\n",
        "print(\"üîÑ Cargando y preparando datos del segmento D...\")\n",
        "\n",
        "try:\n",
        "    # Cargar German Credit Data como proxy del segmento D\n",
        "    german_features, german_targets = data_loader.load_uci_dataset(144, \"german_credit\")\n",
        "    segment_d_data = pd.concat([german_features, german_targets], axis=1)\n",
        "    \n",
        "    print(f\"‚úÖ Datos del segmento D cargados: {segment_d_data.shape}\")\n",
        "    print(f\"   Features: {german_features.shape[1]}\")\n",
        "    print(f\"   Target distribution: {german_targets.iloc[:, 0].value_counts().to_dict()}\")\n",
        "    \n",
        "    # Preprocesar datos\n",
        "    print(\"\\nüîß Preprocesando datos...\")\n",
        "    processed_data = data_preprocessor.preprocess_data(segment_d_data, target_col='class', fit=True)\n",
        "    \n",
        "    print(f\"‚úÖ Datos preprocesados: {processed_data.shape}\")\n",
        "    print(f\"   Feature names: {len(data_preprocessor.get_feature_names())}\")\n",
        "    \n",
        "    # Separar features y target\n",
        "    target_col = 'class'\n",
        "    X = processed_data.drop(columns=[target_col])\n",
        "    y = processed_data[target_col]\n",
        "    \n",
        "    print(f\"\\nüìä Datos preparados:\")\n",
        "    print(f\"   Features (X): {X.shape}\")\n",
        "    print(f\"   Target (y): {y.shape}\")\n",
        "    print(f\"   Target distribution: {y.value_counts().to_dict()}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error cargando datos: {e}\")\n",
        "    X, y = None, None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Divisi√≥n de datos para entrenamiento y evaluaci√≥n\n",
        "if X is not None and y is not None:\n",
        "    print(\"‚úÇÔ∏è Dividiendo datos en train/validation/test...\")\n",
        "    \n",
        "    # Dividir datos\n",
        "    splits = data_splitter.split_data(X, y, stratify=True)\n",
        "    \n",
        "    X_train, y_train = splits['train']\n",
        "    X_val, y_val = splits['validation']\n",
        "    X_test, y_test = splits['test']\n",
        "    \n",
        "    print(f\"‚úÖ Datos divididos:\")\n",
        "    print(f\"   Train: {X_train.shape[0]} muestras\")\n",
        "    print(f\"   Validation: {X_val.shape[0]} muestras\")\n",
        "    print(f\"   Test: {X_test.shape[0]} muestras\")\n",
        "    \n",
        "    # Mostrar distribuci√≥n del target en cada split\n",
        "    print(f\"\\nüìä Distribuci√≥n del target:\")\n",
        "    for split_name, (X_split, y_split) in splits.items():\n",
        "        print(f\"   {split_name}: {y_split.value_counts().to_dict()}\")\n",
        "    \n",
        "    # Informaci√≥n de los splits\n",
        "    split_info = data_splitter.get_data_info(splits)\n",
        "    print(f\"\\nüìã Informaci√≥n de splits:\")\n",
        "    for split_name, info in split_info.items():\n",
        "        print(f\"   {split_name}: {info['n_samples']} muestras, {info['n_features']} features\")\n",
        "    \n",
        "else:\n",
        "    print(\"‚ùå No hay datos disponibles para dividir\")\n",
        "    splits = None\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
